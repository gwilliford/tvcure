\documentclass{article}
\usepackage{./sty/dissformat}
\usepackage{./sty/notation}

\begin{document}
\section{Introduction to Mixture Cure Models}
\begin{comment} Examples of when cure models might be useful
%In political science, potential examples of phenomena where long-term survivors are prevalent abound. For example, in studies of the duration of international peace agreements, many disputants will not fight each other again (see replication analysis below). In studies of how the United States Senate takes to confirm executive nominees \cite[e.g.][]{ostrander2015}, some executive nominees will never be confirmed. Likewise, in studies of policy adoption among U.S. states, some states may never adopt the particular policy in question. 
\end{comment}
\begin{comment} Bias Extensions
To illustrate the potential for long-term survivors to influence the estimates, consider the general formula for the likelihood function of a parametric survival model given by
\begin{align}
L =&\prod_{i=1}^n\{f(t)\}^{C_i}\{S(t)\}^{1-C_i}.
\label{Eqn: Generic Censored Likelihood Function}
\end{align}
In this formulation, $C_i$ acts as a switch that determines whether an observation contributes to the overall likelihood function via the failure density or the survivor function. Thus, while censored observations do not fail, they still contribute information to the likelihood function. 
%In doing so, these observations influence the estimates of both the regression coefficients and the baseline hazard function.
For observations that are destined to eventually fail, incorporating their values into the likelihood function in this way is desirable. 
%Leaving out the censored observations introduces bias/ selects on the dv
%inefficiency
However, if some of the observations treated as censored are actually unlikely to fail, allowing them to contribute information to the likelihood function potentially creates biased estimates of 

Similar problems emerge when including cured observations in the Cox model. Consider the partial likelihood estimator for the Cox proportional hazards model given by
\begin{align}
L_p=\prod_{i=1}^N\bigg[\frac{\exp(\mathbf{x_i\beta)}}{\sum_{j\epsilon R(t)}\exp(\mathbf{x_j\beta})}\bigg]^{C_i},
\label{Eqn: Cox Partial Likelihood}
\end{align}
where $R(t)$ denotes the \textit{risk set}, i.e., the complete set of observations that are at risk of experiencing failure at time $t$. In this formula, $C_i$ acts as a switch that determines whether a given unit contributes directly to the overall likelihood function.  Although right-censored units do not contribute a unique product term to the overall likelihood function, the denominator is calculated with respect to all observations in the risk-set in a given period. Thus, units that are right-censored but still under observation at time $t$ contribute information to the denominator and are thus considered in the estimation of the overall likelihood of failure in a given period. As a result, estimating a model on a dataset that contains a large number of long-term survivors will inflate the size of the denominator, thereby decreasing the contribution of each observed failure to the overall likelihood function. 
%This causes the overall estimate of the hazard to fall for all observed failure, and ultimately creates the potential for downward biased coefficients.
%Consider the following general form of the likelihood function for discrete data
%\begin{align}
%L = \prod_{i=1}^{n}\{f(t_j)\}^{C_i}\{S(t_j)\}^{1-C_i}\label{}
%\end{align}
%where $t_j$ denotes a discrete interval of time from $t_1...t_n$, $f(t_j)$ is the unconditional failure rate, and $S(t_j)$ is the survivor function. Formulated in this way, it is easy to see that observations that are censored at $t_j$ ($C_i=0$) contribute only to the estimation of the survivor function at $t_j$, while observations that fail in a given period ($C_i=1$) contribute only to the estimation of the failure rate.
%Observations that are right-censored (i.e. $C_i=0$ for all $t_j$). 
\end{comment}
\begin{comment} Censoring}
%The first and most common cause of observing a zero is censoring. 
%because they dropout of a study, 
%Alternatively, a unit may no longer be at risk of experiencing the event.
%Observations are considered right-censored if they experience the event after the period of observation ends.
%Censoring emerges because...
% Instead, mixture models assume that whether an individual experiences the event is a function of a latent variable $Y_i$ that equals one if a unit will eventually experience the event of interest and zero if not. Although we cannot directly observe $Y_i$, we can estimate the probability that an observation belongs to the class that will eventually fail. Let $\di$ be a failure or censoring indicator coded 1 if the individual experiences the event and 0 if not.
\end{comment}
\begin{comment} Examples of cure models in other fields
% Cure models have been applied widely in fields including public health \citep{steele2003}, finance \citep{hujer2004}, criminology \citep{schmidt1988, schmidt1989}, medicine \citep{yu2004}, and management science \citep{deleonardis2014}.
\end{comment}

% Assumptions
% Independent/noninformative/random censoring and independent of Y

% From: beger2017, The presence of a large sub-population which is not at risk for an event, will in practice inflate estimates of the survival fraction, and reduce hazard estimates for all subjects. This is the case because the underlying risk is estimated based on subjects that genuinely will fail and those that are cured. Hence, such a model will over predict the hazard for subjects that are not at risk (cured), and under predict for those who are at risk of eventually experiencing the event of interest.

% The Problem wiht Cured Subjects
%In many cases, the assumption that all individuals will fail is theoretically sound. In studies of mortality, all individuals will eventually die. Similarly, in studies of leader tenure, all leaders will eventually leave office in one manner or another.
%Studies of tenure duration are justified in assuming that all cabinets will eventually fail.

%In other cases, however, the assumption that all units will eventually experience the event of interest may not be tenable. For example, in the study of time until cancer relapse, many individuals will not experience a recurrence \citet[e.g.][]{sposto2002}.  In biostatistics and related fields, these observations are often referred to as \textit{cured} individuals, since they have cannot experience the event of interest again. In other contexts, these observations are often referred to as \textit{long-term survivors}.

Standard survival models make the assumption that all units eventually fail. However, this assumption is unrealistic in some cases where some observations are not expected to fail. To fix ideas, let $Y$ be an indicator coded 1 if the individual will eventually experience the event and 0 if it will not. Although cases that are observed to fail must belong to the class of uncured individuals $Y = 1$, censored cases may belong to either class. Let $p$ represent the probability that an individual will eventually experience the event, $\Pr(Y = 1)$. The probability that an individual is ``cured'' is thus given by $1-p$. Let $T$ indicate the time until the event occurs, which is only defined for individuals who experience the event ($Y = 1$), and let $f(t | Y = 1)$ represent the conditional failure density, $h(t | Y = 1)$ represent the conditional hazard rate, and $\su$ represent the conditional survival function of $T$ for uncured individuals. Let $\di$ represent a failure or censoring indicator coded 1 if the individual experiences the event and 0 if it is censored. 
\begin{comment} Old notation
%To fix ideas, let $T$ denote a positive continuous random variable describing survival times and $t$ denote a particular realized value of $T$. Let $f(t)$ represent the unconditional probability of failure at time $t$ [$\Pr(T_i=t)$] and $S(t)$ represent the survivor function defined as the probability that a unit survives at least until time $t$ $[\Pr(T_i\geq t)]$. Additionally, let $\delta_i$ be an indicator variable equal to one if a unit is observed to fail during the observation period and zero otherwise. 
%We assume an independent, noninformative, random censoring model and that censoring is statistically independent of Y.
\end{comment}

% Bias

Including cured subjects in statistical analyses has the potential to produce bias in the coefficient estimates. The fundamental problem with cured observations lies in the fact that cured individuals are observationally indistinguishable from censored individuals, and therefore cannot be removed from the dataset. Although censored subjects (including cured subjects) do not contribute directly to the estimate of the failure rate in survival models, they do contribute to the estimation of the baseline hazard rate in parametric models and the risk set in Cox models. Normally this is considered good practice, since failure to incorporate information from these observations introduces the potential for selection bias and needlessly discards useful information (i.e. individuals with a particular covariate profile last \textit{at least} until the censoring period). However, if some units are not destined to fail, using them to estimate the duration of events that will fail introduces the potential for bias. In essence, this can be thought of as a form of unobserved heterogeneity which may result in biased coefficient estimates for those units that actually are at risk. In practice, including these subjects in the dataset inflates the estimates of the survival times (and deflates the estimates of the hazard rate), since cured subjects will have long survival times that exceed the failure times of any of the other observations in the dataset.
%(in the baseline hazard rate or the number of units actually at risk

In order to account for these problems, researchers have developed cure models (also known as or models for long-term survivors). The most common method of dealing with these issues in social scientific applications is through mixture models or split-population models.
%Introduced by \citet{anscombe1961}, 
These models assume that the sample is composed of individuals drawn from two separate theoretical subpopulations, one which is at risk of experiencing the event of interest and one which is not.\footnote{An alternative class of models known as \textit{nonmixture} cure models corrects for these problems by imposing an upper bound on the cumulative hazard rate. Although these models have been used widely in biostatistics, they have seen limited applications in social scientific fields and are not considered further here.}
\begin{comment}%More content for the footnote
%\textit{bounded cumulative hazard} models. These models impose an asymptote on the integrated hazard rate. This has the effect of imposing an upward bound on the survivor function, helping to correct the bias associated with estimating}
%These models can be considered as members of a  broader class of mixture models, which also includes so-called \textit{``zero-inflated models''} such as the zero-inflated Poisson \citep{lambert1992} and zero-inflated ordered probit models \citep{harris2007, bagozzi2015}.  In a similar fashion, each of these assumes that some observations are drawn from a latent subclass that has no risk of experiencing the event of interest and estimates a model that incorporates this heterogeneity.
\end{comment}
%These models estimate the probability that an individual will eventually fail and weight their contributions to the estimation of duration times accordingly. Because some individuals that are not long-term survivors are not observed to fail during the observation period due to right-censoring, we cannot directly observe which subpopulation an individual belongs to.
Mixture cure models correct for the presence of cured observations by positing a marginal survival function for the full population of individuals of the form 
\begin{align}
S_{pop}(t) = pS(t|Y = 1) + 1 - p.\label{eq_spop}
\end{align}
%Cure models assume that separate processes determine whether an event will occur, known as the incidence, and when it will occur, known as the latency.
The probability of failure, $p$, is typically referred to as the incidence, while the conditional survival function is typically known as the latency. Here, $p$ is used to weight the contributions of different subjects to the survival function. Note that if $p = 1$ for all observations in the dataset, the cure model reduces to a standard survival model. 

% Covariates
Covariates can be incorporated into \eqref{eq_spop} by modeling $p$ as a function of covariates $z$ and their associated coefficients $\gamma$ and modeling $\su$ as a function of covariates $x$ and coefficients $\beta$. The incidence, $p$, is typically modeled using a logit link function, producing
\begin{align}
p(\mathbf{z}) = \Pr(Y = 1;\mathbf{z}) = \frac{\ezg}{1 + \ezg}.
\end{align}
However, $p$ may also be modeled using alternative link functions appropriate for binary response models, such as the probit or complementary log-log links.
Since covariates are allowed to influence both the incidence and latency components of the model, analysts can examine the effect of a particular covariate on both components of the model separately. This enables us to determine what effect a particular covariate has on both the probability of failure (or cure) and the effect of that same covariate on the hazard rate, conditional on it's effect on the probability of failure.

The latency component can be modeled using either parametric or semiparametric models for $\su$. Parametric cure models involve specifying a parametric distribution for the hazard rate. Various parametric distributions have been used by previous scholars, including the Weibull, log-logistic \citep{hettinger2005}, log-normal, etc. %TODO - Citations here
Once a parametric distribution has been chosen, a likelihood function can be constructed of the form 
\begin{align}
\L = \prod_{i=1}^n[p_if(t_i|Y = 1)]^{\delta_i}\{(1-p_i)+p_i\su\}^{1-\delta_i}.
\end{align}
Since parametric approaches are discussed at length elsewhere (e.g. \cite[][Chapter 9]{box2004a}, \cite{beger2017}), they are not expounded upon further here. 

Although parametric approaches are easier to estimate, it is difficult to verify that a particular parametric distribution is appropriate.
Moreover, using an inappropriate distribution constitutes misspecification bias. As such, without strong theory or evidence to support the use of a particular distribution, semiparametric approaches that leave the baseline hazard unspecified are a safer alternative. 
%\subsection{Discrete Time Cure Models}
To address this problem, \citet{kuk1992} developed the semiparametric cure model. Their model uses a standard Cox proportional hazards model to model the latency component of \eqref{eq_spop} by assuming that $\hux = \ho\exp(\xvec'\bvec)$, where $\mathbf{x_i}$ is a vector of covariates for individual $i$, $\bvec$ is a vector of unknown regression parameters, and $\ho$ is the conditional baseline hazard function. For individuals that will eventually fail, the conditional survival function in \eqref{eq_spop} is given by
\begin{align}
\su = \so^{\exb}\label{eq_scond}
\end{align}
where $\so$ is the conditional baseline survival function.
%\begin{align}
%S(t | x) = \so^{\exp(\xib)}
%\end{align}

%\begin{align}
%S(t | \xvec, \zvec) = \p S_{u0}(t)^{exp(xib)} + 1 - p
%\end{align}

% Time-Varying Covariates
The mixture cure model presented in can easily be extended to allow for the use of time-varying covariates and stratification. Time-varying covariates can be incorporated by using the ``counting process'' or ``start-stop'' data structure to include one observation for each subject in the riskset at each of the observed failure times, $t_{(j)}$ \citep[see, e.g.,][Chapter 7]{box2004a}. However, incorporating time-varying covariates in the model fundamentally changes the interpretation of the incidence portion of the model. Rather than merely identifying which individuals are and are not immune, the inclusion of time-varying covariates introduces introduces the possibility that subjects may be immune at some times and not at others \citep{beger2017}. Instead, it may be preferable to include only time-invariant covariates in the incidence component of the model \citep{dirick2017}.
%TODO - fill in chapter for BSJ cite
%TODO - Time-varying immunity - in addition, it may make sense to include
%TODO - Simultaneity bias
\begin{comment} Where zi is a vector of covariates for a subject at a given time. For interpretation, it is important
to note that with time-varying covariates, the risk (or cured) estimate for a subject is particular to a
given time point rather than being constant over all time periods in the spell.2 Depending on the
covariates, the risk estimate for a subject can thus fluctuate over time. To ease interpretation, it might
be convenient to restrict covariates in the logit risk model to slow-moving, stable covariates in order to
produce stable risk estimates for subjects.
\end{comment}
	
\section{Estimation of the Semiparametric Cure Model}
% Full likelihood
Since the distribution of $\su$ is left unspecified, maximizing the full likelihood of the semiparametric cure model using standard Newton-Raphson type algorithms is not possible. Standard Cox proportional hazards models make use of the partial likelihood method to eliminate $\su$ from the likelihood. However, due to the more complex form of the semiparametric PH cure model, this is not possible.
% Partial likelihood
%Note also that, since $\wi$ depends on $\so$, the baseline function is involved in the estimation of b and \bvec.

As such, semiparametric cure models require an estimation technique that can maximize the full-likelihood and provide estimates of $\su$. \citet{peng2000, sy2000} suggested using an expectation-maximization (EM) algorithm.\footnote{Other scholars have suggested alternative estimation strategies using Markov Chain Monte Carlo simulations \citep{kuk1992}, multiple imputation \citep{lam2005}, and Bayesian techniques \citep{}.}% these are not considered further here. 
The complete data log-likelihood is given by
\begin{align}
\L_C(\bvec, \gvec, H_0) =& \prod_{i = 1}^{n} p_i^{y_i} (1 - p_i)^{1-y_i} \prod_{i = 1}^{n} \{\hux^{\di\yi}\sux^{\yi} \},\label{eq_lik_full}
%\{h_0(t_i | Y = 1)\exp(\xib) \}^{\di\yi} \times \exp(-\yi H_0(t_i|Y=1)\exp(\xib).\label{eq_LC}
\end{align}
where the first product term contains the parameters related to the incidence component of the model and the second contains the parameters related to the latency component. The log of \eqref{eq_lik_full} can be expressed as the sum of two likelihood functions, $\l_{C1}$ and $\l_{C2}$, given by
\begin{align}
\l_1(\bvec) =& \sum_{i = 1}^{n}\bigg\{\yi \log(\p) + (1-\yi)\log[1-\p] \bigg\} \label{eq_lik_l1},\\
\l_2(\gvec, \su) =& \sum_{i = 1}^{n}\bigg\{\yi\di\log\hux + \yi\log[\sux] \bigg\}\label{eq_lik_l2}.
%l1 = SUM[y * log(p) + (1-y)log(1-p)]
%l2 = SUM{(dy)log(hazard) + ylog[S(t)]}
\end{align}

The E-Step of the EM algorithm takes the conditional expectation of the complete log-likelihood function with respect to the unobserved $y_i$ values for the given current estimates of $\bvec$, $\gvec$, and $\so$.\footnote{Initial estimates for $w_i$ are derived by setting all censored cases to 0 and all uncensored cases to 1.} Although $y_i$ is not observed, the expectation of $\yi$ conditional on the observed data and current parameter estimates is sufficient to conduct this step since \eqref{eq_lik_l1} and \eqref{eq_lik_l2} are linear functions of $\yi$.
%Although $y_i$ is unobserved, the conditional expectation of $y_i$ is enough to maximize \eqref{eq_lik_l1} and \eqref{eq_lik_l2}\ since both are linear functions of $y_i$. The conditional expectation of $y_i$ is
%Doing so produces the following equations. Let $\Theta$ denote the parameters to be estimated % This needs to be reworded, too close to Sy and Taylor
%For uncensored individuals, $\yi$ is observed, and the expected value is equal to 1.
%\begin{align}
%E(Y_i | \di = 1, \ti, \xvec, \zvec) = \yi = 1.\label{eq_expecy_uncens}
%\end{align}
%For censored individuals, the conditional expectation of $\yi$ is given by the probability that individual $i$ is uncured (i.e. the probability of surviving until time t) time the probability of being uncured divided by the probability of being cured + the above).
%\begin{align}
%E(\yi|\di = 0, \ti, \xvec, \zvec) = \bigg(\frac{p_i\sox)}{1-p_i + p_i\sox}\bigg)\label{eq_expecy_cens}
%\end{align}
%Equations \eqref{eq_expecy_uncens} and \eqref{eq_expecy_cens} can be combined to produce 
Let $\wi$ denote the conditional expectation of $\yi$ given the current parameter estimates $\Theta^{\{m\}} = (\bvec^{\{m\}})$, given by
\begin{align}
\wi = E(\yi|\di, \ti, \xvec, \zvec) = \delta_i + (1-\delta_i)\bigg(\frac{p_i\sox}{1-p_i + p_i\sox}\bigg).
\end{align}
For uncensored individuals ($\di = 1$), the value of $\yi$ is known and $\wi$ reduces to 1. For censored individuals, $\wi$ reduces to the probability of the $i^{th}$ censored individual being uncured.  Put otherwise, for censored individuals, $\wi$ ``represents a fractional allocation to the susceptible group,'' \citep[][p. 229]{sy2000}. Substituting $\wi$ for $\yi$ in \eqref{eq_lik_l1} and \eqref{eq_lik_l2} produces
\begin{align}
\l_1(\bvec) =& \sum_{i = 1}^{n}\bigg\{\wi \log(\p) + (1-\wi)\log[1-\p] \bigg\} \label{eq_lik_l1w},\\
\l_2(\gvec, \su) =& \sum_{i = 1}^{n}\bigg\{\wi\di\log\hux + \wi\log[\sux] \bigg\}\label{eq_lik_l2w}.
\end{align}

The M-step involves maximizing the log-likelihood functions of \eqref{eq_lik_l1w} and \eqref{eq_lik_l2w} with respect to the unknown parameters $\bvec$, $\gvec$, and $\so$ using the current values of $\wi$. Since \eqref{eq_lik_l1w} does not depend on the value of $\bvec$ or $\so$, estimates of $\gvec$ can be obtained by maximizing \eqref{eq_lik_l1w} using standard binomial regression routines. Similarly, since \eqref{eq_lik_l2w} does not depend on the value of $\gvec$, estimates of $\bvec$ and $\so$ can be obtained by maximizing \eqref{eq_lik_l2w} using standard Cox PH routines \citep{peng2003, cai2012}, where $\so$ is estimated using \citet{breslow1972}'s version of the Cox PH model, given by
\begin{align}
\hat{S}_0(t| Y = 1) = \exp\bigg(-\sum_{j:t_{(j)}\leq t}\frac{d_{t_{(j)}}}{\sum_{i \epsilon R(t_{(j)})}\wi\exp(\xvec'\hat{\bvec}) }\bigg),\label{eq_so} 
\end{align}
where $d_{t_{(j)}}$ is the number of events at time $t_{(j)}$ and $R(t_{(j)})$ is the set of observations that are at risk of failure at $t_{(j)}$.\footnote{This constitutes using a modified version of the Nelson-Aalen estimator to estimate the baseline cumulative hazard and then estimating the survivor function using $\so = \exp-\hat{H_0(t|Y=1)}$.}
\begin{comment}
% Estimating the survival function
Once estimates of $\bvec$ have been obtained, estimates of the conditional baseline survival function, $\so$, are obtained using profile likelihood methods. This typically involves using a modification of \citet{breslow1972}'s likelihood for the Cox PH model.\footnote{\citet{sy2000} demonstrate that $\su$ can also be estimated using the nonparametric full likelihood method of \citet{kalbfleisch1980}.} 
\end{comment}

% zero tail constraints
The estimator defined in \eqref{eq_so} may not approach 0 for $t$ greater than the maximum observed failure time, $t_{(k)}$. \citet{taylor1995} characterizes this as an identifiability problem in which the tail of the $\su$ distribution is difficult to estimate. \citet{taylor1995} suggested imposing the constraint that $\so$ = 0 for $t > t_{(k)}.$ This constraint is achieved by setting $\wi = 0$ for observations where $t > t_{(k)}$ in the E-step. This effectively eliminates the identifiability problem and leads to more stable parameter estimates and faster convergence \citep{taylor1995}. In addition, this constraint may lead to less biased estimates of $\bvec$ and $\gvec$ in the presence of high levels of censoring. Substantively, this constraint implies that most of the subjects left at the end of the observation period are members of the cured group and are unlikely to fail in the future. Put otherwise, the use of the semiparametric PH mixture cure model may not be appropriate when the follow-up period of a study is not long enough for most of the susceptible individuals to have already failed. % ill-beahved likelihood function
Once the estimates of $\bvec$, $\gvec$, and $\so$ are obtained, the E-step is repeated using the newly obtained estimates to re-estimate the value of $\wi$. Estimation proceeds by iterating between the E and M steps until the values of the parameters converge. 

%\subsection{Variance Estimation}
% Cai et al.: Because of the complexity of the estimating equation in the EM algorithm, the standard errors of estimated parameters are not directly available. In order to obtain the variance of β̂ and b̂, this package randomly draws bootstrap samples with replacement by ‘sample’ function in R. The default number of bootstrap is set to be 100. We show that there is little difference in standard errors when using 100, 200 and 500 bootstrap samples in the illustrated examples (Tables 1 and 2), which means that 100 is enough for those two datasets.
% Variance Estimation
\citet{fang2005} demonstrate that the maximum likelihood estimates of $\su$ are consistent and asymptotically normally distributed.
Since estimates of the variance of the estimated parameters are not directly available from the EM algorithm, alternate methods of estimating the standard errors are necessary for hypothesis testing. 
Various authors have derived approximations of the standard errors, including \citet{peng2000, sy2000, fang2005, xu2014}. However, since
%peng2000 use an approximation 
%sy2000 derive an approximation using the observed information matrix 
%fang2005 profile likelihood approach
%xu2014
%However, since these numerical instability
%Cannot be easily be adapted to accommodate variations or extensions of the semiparametric PH mixture cure model.
%sy2000 method underestimates variance when the number of failures is few or there is heavy censoring
As such, most existing software packages use a nonparametric bootstrap with replacement to estimate the standard errors \citep[e.g.][]{peng2003, cai2012}. 
% question: bootstrapping --- clusters? --- no, b/c only one contributes at any given time --- what about logit?

\subsection{Extensions}
The semiparametric PH cure model can be extended to accommodate a number of other issues that are common with survival data. Stratification can be used to allow the conditional baseline hazard to vary by groups of observations \citep{peng2003}. In addition, analysts can allow covariates to have a non-linear additive effect on the incidence component by using generalized additive models in place of a binomial generalized linear model for $\gvec$ \citep{peng2003, ramires2018}. Non-linear effects may also be incorporated using the univariate transformations of the covariates \citep[see][]{therneau2000}. 

\citet{wu2014} develop a version of the semiparametric PH mixture cure model that allow analysts to incorporate information on the cure status of some subjects when a subset of the censored subjects is available. In addition, variants of the semiparametric PH mixture model have been developed that can accommodate interval-censored data \citep{liu2009, hu2013, lam2013}, frailty terms \citep{price2001, peng2008, peng2008a}, and covariate and time-dependent censoring \citep{lu2004a, othus2009}.
%We assume an independent, noninformative, random censoring model and that censoring is statistically independent of Y. 

\subsection{Issues and Diagnostics}
% Convergence / separation /problems in small samples
Cure models are prone to issues of non-convergence or unstable coefficient estimates, particularly in small samples or in samples in which there are very few failures or very few cured observations (although the constraint on the survivor function discussed above improves the performance of the cure model in this regard \citep{taylor1995, sy2000}). In addition, cure models may be prone to complete or quasicomplete separation (i.e. infinite coefficient estimates) in either the incidence or latency components. This is particularly likely in small samples and when there are very few failures or very few cured subjects \citep{sy2000}. Although this is difficult to deal with without omitting covariates from the model, the use of Bayesian priors to ameliorate these issues is an area of active research \citep{han2017}. 
% Event size
%In datasets with few failures, the parameter estimates of $\bvec$ may be unstable \citep{sy2000}. 
%Model specification
%May also be unstable if there are few cured individuals that survive censoring until the end of the observation period to get a reasonable estimate of the incidence proportion. % THis is common in social science, as most observations are right-censored
%The zero-tail constraint also improves the performance of the model in regard to both of these
%Zero-tail constraint 
%	Improves performance of the model
%	Improves performance in small samples



% Identification
% PH assumption 
Since the semiparametric proportional hazards cure model is dependent on the proportional hazards assumption, verifying that the covariates conform to the proportional hazards assumption is important. 
% Diagnostics
Variants of the standard proportional hazards diagnostics used with Cox models have not yet been fully developed for cure models, although the development of residual-based methods for assessing model fit is an area of active research \citep{peng2017, ramires2018}. Instead, the use of graphical techniques such as log-log survivor plots may be used to assess whether a covariate's effect is proportional over time. Covariates that do not meet this assumption may be dealt with in the usual ways, e.g. by stratifying on the offending covariate or by incorporating interactions with the log of time. Although the proportional hazards assumption holds for the conditional survivor function, it does not necessarily hold for the population survival function, $\spop$.

\subsection{Comparison of parametric and semiparametric duration models}
\citet{kuk1992, sy2000} conduct Monte Carlo simulations to compare the performance of a Weibull cure model and the semiparametric cure model when at various levels of censoring when the Weibull distribution is known to be an appropriate distribution.\footnote{Both studies use an exponential distribution to model the hazard rate, which is a special case of the Weibull distribution.} Across all levels of censoring, they find that the semiparametric cure model produces more efficient estimates of the incidence parameters, $\gvec$. With low levels of censoring, the semiparametric cure model provides only slightly less efficient estimates of $\bvec$ than the parametric model. However, at high levels of censoring, the semiparametric model tends to actually provide more efficient estimates of $\bvec$ as well.



\section{Analysis of International Ceasefire Duration}
%%% Reason why Cox model isn’t adequate -----------------------------------------------------------------------------------------------------------------------------------


%% Description of the Data
In order to demonstrate the usefulness of the semiparametric proportional hazards cure model, I replicate an analyze data on interstate cease-fire agreements from \citet{fortna2004b, lo2008, werner2005}. This dataset covers a total of 188 international cease-fires signed from 1914 to 2001. The dataset includes one observation for each cease-fire year. The dependent variable each of these authors analyzes is the duration of the cease-fire agreement, which is coded as failing when two states resume hostilities. Cease-fires are treated as censored if the cease-fire has not been violated at the end of the dataset or if one state ceases to exist.\footnote{See \citet{fortna2004b, lo2008} for a more extensive discussion of how particular ceasefires are coded.} A description of the independent variables and their descriptive statistics is provided in the appendix.

%% Theoretically Some Peace Agreements Will Not Fail
Although each of the analyses above uses standard survival models to assess the influence of various covariates on the duration of cease-fires, there is reason to question whether these models are appropriate for modeling the data-generating process that underlies the observed durations. In many of these cases, the disputants are likely to reach an agreement that resolves the underlying cause of the conflict and, as a result, these cease-fires are unlikely to fail over the long term. The use of standard survival models only speaks to whether some agreements last longer than others, rather than whether they endure over the long term. Without modeling this data-generating process explicitly, the estimates of the parameters will be biased and inconsistent, therefore introducing the possibility of drawing incorrect inferences. Moreover, from a substantive perspective, these models only demonstrate that the covariates extend the duration of a ceasefire. As such, they are unable to speak to whether agreements are likely to produce permanent peace between the disputants.

%% Overestimate substantive effects


%% Kaplan Meier
A preliminary analysis of the data confirm that many cease-fires are unlikely to fail within any reasonable amount of time. Of 188 cease-fires in the dataset, only 56 (29.8 percent) experience failure during the observation period. This can be further illustrated by examining the nonparametric Kaplan-Meier estimate of failure time presented in Figure \ref{fig:lhrkm}. Less than 40 percent of ceasefires are expected to fail within 90 years of being implemented. Since a substantial number of cease-fires do not fail within a century of being signed, it is difficult to justify the assumption that all censored cases will eventually fail (over any reasonable time period). This provides strong evidence that many of the cases may be considered immune to conflict recurrence, indicating that the use of a cure model is appropriate. 

	\begin{figure}[htbp]\centering
		\caption{Kaplan-Meier estimate of the duration of international cease-fires from 1914-2001 (with ninety-five percent confidence intervals).}
		\includegraphics[width = 4in]{./img/lhrkm}
		\label{fig:lhrkm}
	\end{figure}

%%% Results -----------------------------------------------------------------------------------------------------------------------------------
Conflicts that end without resolving uncertainty over the disputants' capabilities or resolve are more likely to recur 
Whether two states remain at peace in the long term depends on whether the conflict successfully clarifies the range of acceptable outcomes between disputants. 
- Battle deaths - costs
- Ties and battle tide

Root vs. proximate causes
Some cease-fires are inherently unstable and thus prone to being disrupted. Some cease-fires are less likely to see the recurrence of information or commitment problems and thus less likely to break down in the future.

Size of the bargaining range
Strength of credible commitments

Finally, states that do not interact on a regular basis are unlikely to find themselves involved in highly salient disputes that justify the costs of conflict. 
% Uncertainty
% Strenght of Uncertainty
Second, factors that 


\section{Appendix}
\include{./tab/descvars}
\include{./tab/descstats}
\end{document}